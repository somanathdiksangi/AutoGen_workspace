{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077562bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6d50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model_clind=OllamaChatCompletionClient(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ec0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responce=await ollama_model_clind.create([UserMessage(content=\"hello what is autogen\",source=\"user\")])\n",
    "    print(responce)\n",
    "    await ollama_model_clind.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8010b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb44ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=32, completion_tokens=8) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2:1b\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "print(response)\n",
    "await ollama_model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeeb0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d8b6a",
   "metadata": {},
   "source": [
    "## first autogen agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402778b",
   "metadata": {},
   "source": [
    "form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe9ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ba81f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b92dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autogen in the context of video games, particularly flight simulators, refers to **procedurally generated terrain, objects, and other environmental elements**.  It's a way of creating a vast and detailed world without having to manually model every tree, building, and road.\n",
      "\n",
      "Instead of static models, autogen uses algorithms to create these elements based on predefined rules and parameters. This results in a dynamic and believable environment that feels more natural and less repetitive than a fully hand-crafted one, especially at lower altitudes or in areas of less detail.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=\"your_api_key\",\n",
    ")\n",
    "\n",
    "response = await model_client.create([UserMessage(content=\"What is the autogen\", source=\"user\")])\n",
    "print(response.content)\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf8c83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=\"AIzaSyDiQAlcGI7UsGIj2t1x995wrItUfFyqwrY\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1ac2921",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_agent=AssistantAgent(name=\"kalu\",model_client=model_client )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b0eb689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextMessage(source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 6, 20, 11, 8, 13, 18247, tzinfo=datetime.timezone.utc), content='tell a joke', type='TextMessage'), TextMessage(source='kalu', models_usage=RequestUsage(prompt_tokens=91, completion_tokens=18), metadata={}, created_at=datetime.datetime(2025, 6, 20, 11, 8, 13, 782475, tzinfo=datetime.timezone.utc), content='Why did the bicycle fall over?\\n\\nBecause it was two tired!\\n\\nTERMINATE\\n', type='TextMessage')]\n"
     ]
    }
   ],
   "source": [
    "result= await my_first_agent.run(task=\"tell a joke\")\n",
    "print(result.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f43713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\\n\\nTERMINATE\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913a165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
