{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e73eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent,UserProxyAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.conditions import TextMessageTermination,MaxMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "import asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "api_key=os.getenv(\"GAMINI_API_KEY\")\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ec43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92f75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_agent = AssistantAgent(\n",
    "    name='PlanningAgent',\n",
    "    description = 'An agent for planning tasks, this agent should be first to engage when given a new task',\n",
    "    model_client=model_client,\n",
    "    system_message='''\n",
    "    You are a planning agent\n",
    "    your job is to break down complex tasks into smaller, manageable substasks.\n",
    "    You team members are :\n",
    "    WebSearchAgent : Searches for Information.\n",
    "    DataAnalystAgent : Perform Claculation.\n",
    "\n",
    "    Your only plan and delegate tasks - you don not execute yourself.\n",
    "\n",
    "    When assigning the tasks, use the below format :\n",
    "    1. <agent> : <task>\n",
    "\n",
    "    After all the tasks are completed, summarize the findings and print out 'TERMINATE'.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae5d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web_tool(query:str)->str:\n",
    "    ''' \n",
    "    '''\n",
    "    # Simulating Web search\n",
    "    if ('2006-2007') in query:\n",
    "        return ''' \n",
    "        Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
    "        Udonis Haslem: 844 points\n",
    "        Dwayne Wade : 1397 points\n",
    "        James Posey : 550 points        \n",
    "    '''\n",
    "    elif \"2007-2008\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
    "    elif \"2008-2009\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
    "    return \"No data found.\"\n",
    "\n",
    "\n",
    "web_search_agent = AssistantAgent(\n",
    "    name = 'WebSearchAgent',\n",
    "    description= 'An agent for searching the web for information.',\n",
    "    model_client=model_client,\n",
    "    tools = [search_web_tool],\n",
    "    system_message='''\n",
    "        You are a web search agent.\n",
    "        Your only tool is search_web_tool - use it to find the information you need.\n",
    "\n",
    "        You make only one search call at a time.\n",
    "        \n",
    "        Once you have the results, you never do calculations or data analysis on them.\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbef1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_change_tool(start:float,end:float) ->float:\n",
    "    if(start==0):\n",
    "        return 0\n",
    "    return ((end-start)/start)*100\n",
    "\n",
    "data_analyst_agent = AssistantAgent(\n",
    "    name = 'DataAnalystAgent',\n",
    "    description= 'An agent for performing calculations and data analysis.',\n",
    "    model_client=model_client,\n",
    "    tools= [percentage_change_tool],\n",
    "    system_message='''\n",
    "        You are a data analyst agent.\n",
    "        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
    "\n",
    "        If you have not seen the data, ask for it.\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39357ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import TextMentionTermination,MaxMessageTermination\n",
    "combined_termination = TextMentionTermination('TERMINATE') | MaxMessageTermination(max_messages=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f6c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = '''  \n",
    "Select and agent to perform the task.\n",
    "{roles}\n",
    "\n",
    "Current conversation history:\n",
    "{history}\n",
    "\n",
    "Read the above conversation and then select and agent from {participants} to perform the next task.\n",
    "Make sure that the planning agent has assigned task before other agents start working.\n",
    "only select one agent.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bed389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "\n",
    "selector_team = SelectorGroupChat(\n",
    "    participants=[planning_agent,web_search_agent,data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=combined_termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d30a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ' Who was the Miami heat player with the highest point in 2006-2007 season and what was the percentage change in his total' \\\n",
    "'rebounds between 2007-2008 and 2008-2009 seasons ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937324ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "stream = selector_team.run_stream(task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe7361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      " Who was the Miami heat player with the highest point in 2006-2007 season and what was the percentage change in his totalrebounds between 2007-2008 and 2008-2009 seasons ?\n",
      "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
      "[FunctionCall(id='', arguments='{\"query\":\"Who was the Miami heat player with the highest points in 2006-2007 season?\"}', name='search_web_tool'), FunctionCall(id='', arguments='{\"query\":\"What was the percentage change in total rebounds for the Miami heat player with the highest points in 2006-2007 season between 2007-2008 and 2008-2009 seasons?\"}', name='search_web_tool')]\n",
      "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
      "[FunctionExecutionResult(content=' \\n        Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade : 1397 points\\n        James Posey : 550 points        \\n    ', name='search_web_tool', call_id='', is_error=False), FunctionExecutionResult(content=' \\n        Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade : 1397 points\\n        James Posey : 550 points        \\n    ', name='search_web_tool', call_id='', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (WebSearchAgent) ----------\n",
      " \n",
      "        Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
      "        Udonis Haslem: 844 points\n",
      "        Dwayne Wade : 1397 points\n",
      "        James Posey : 550 points        \n",
      "    \n",
      " \n",
      "        Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
      "        Udonis Haslem: 844 points\n",
      "        Dwayne Wade : 1397 points\n",
      "        James Posey : 550 points        \n",
      "    \n",
      "---------- TextMessage (DataAnalystAgent) ----------\n",
      "\n",
      "---------- TextMessage (DataAnalystAgent) ----------\n",
      "```\n",
      "print(\"Miami Heat player with highest points in 2006-2007 season:\")\n",
      "players = {\n",
      "    \"Udonis Haslem\": 844,\n",
      "    \"Dwayne Wade\": 1397,\n",
      "    \"James Posey\": 550,\n",
      "}\n",
      "\n",
      "highest_scorer = max(players, key=players.get)\n",
      "print(highest_scorer)\n",
      "\n",
      "```\n",
      "```\n",
      "print(\"Data for rebound percentage change is needed.\")\n",
      "\n",
      "```\n",
      " I need the data for total rebounds for Miami Heat players in the 2007-2008 and 2008-2009 seasons to calculate the percentage change.  Please provide that data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for WebSearchAgent_a673e466-19b6-46e5-8a7e-dc0f774104d8/a673e466-19b6-46e5-8a7e-dc0f774104d8\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 852, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 981, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\_base_client.py\", line 1762, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\_base_client.py\", line 1562, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\n",
      "Error processing publish message for PlanningAgent_a673e466-19b6-46e5-8a7e-dc0f774104d8/a673e466-19b6-46e5-8a7e-dc0f774104d8\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for DataAnalystAgent_a673e466-19b6-46e5-8a7e-dc0f774104d8/a673e466-19b6-46e5-8a7e-dc0f774104d8\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 605, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 852, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 981, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\_base_client.py\", line 1762, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\_base_client.py\", line 1562, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m Console(stream)\n",
      "File \u001b[1;32mc:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[0m, in \u001b[0;36mConsole\u001b[1;34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m last_processed: Optional[T] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    115\u001b[0m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[0;32m    119\u001b[0m         duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:518\u001b[0m, in \u001b[0;36mBaseGroupChat.run_stream\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message\u001b[38;5;241m.\u001b[39merror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message\u001b[38;5;241m.\u001b[39merror))\n\u001b[0;32m    519\u001b[0m     stop_reason \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 852, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 981, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\_base_client.py\", line 1762, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\acer\\anacondasomu\\envs\\Auto\\Lib\\site-packages\\openai\\_base_client.py\", line 1562, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Please ensure that the number of function response parts is equal to the number of function call parts of the function call turn.', 'status': 'INVALID_ARGUMENT'}}]\n"
     ]
    }
   ],
   "source": [
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cf3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
